#include <object_detection.h>


using namespace std;
using namespace InferenceEngine;


// выключать камеру после проверки

ObjectDetection::ObjectDetection()
{
  image_transport::ImageTransport it(nh);
  image_raw_pub = it.advertise("/monitoring/image_raw", 1);
  image_detected_pub = it.advertise("/monitoring/image_detected", 1);
  boxes_max_pub = nh.advertise<object_detection::BoxMax>("/monitoring/box_max", 1);
  centroid_pub = nh.advertise<object_detection::Centroid>("/monitoring/box_centroid", 1);
  deviation_pub = nh.advertise<object_detection::Deviation>("/monitoring/box_dev", 1);
  //command_topic = nh.subscribe("/telegram_bot/command", 1, &ObjectDetection::commandCallback, this);

  // -----------------------------------------------------------------------------------------------------
  ROS_INFO_STREAM("-------------------------------------------");
  ROS_INFO_STREAM("Object Detection Node Started");
  readInput();
  initNetworks();
  objectInference();
  FLAGS_auto_resize = true;
  // -----------------------------------------------------------------------------------------------------
}

void ObjectDetection::commandCallback(const std_msgs::String::ConstPtr& msg)
{
    is_telegram_online = true;
    telegram_msg = msg->data;
}

ObjectDetection::~ObjectDetection(){}

void ObjectDetection::readInput()
  {
    ROS_INFO_STREAM("-------------------------------------------");
    ROS_INFO_STREAM("Reading input: ");
    ROS_INFO_STREAM("\tInput camera " << cameraID);
    capture.open(cameraID);
    if (!capture.isOpened())
    {
        ROS_ERROR_STREAM("Cannot open camera: " << cameraID);
        exit(0);
    }
    input_width  = (size_t) capture.get(cv::CAP_PROP_FRAME_WIDTH);
    input_height = (size_t) capture.get(cv::CAP_PROP_FRAME_HEIGHT);
    ROS_INFO_STREAM("---Camera was opened successfully");

    // read input (video) frame
    capture >> curr_frame;
    capture >> raw_frame;

  }


void ObjectDetection::initNetworks()
{
  // --------------------------- Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
  ROS_INFO_STREAM("-------------------------------------------");
  ROS_INFO_STREAM("Loading network files: ");
  ROS_INFO_STREAM("\t" << XML_MODEL_NAME);
  ROS_INFO_STREAM("\t" << BIN_MODEL_NAME);
  /** Read network model **/
  netReader.ReadNetwork(XML_MODEL_PATH);
  /** Set batch size to 1 **/
  netReader.getNetwork().setBatchSize(1);
  /** Load model weights **/
  netReader.ReadWeights(BIN_MODEL_PATH);
  ROS_INFO("---Network's files loaded");

  // --------------------------- 3. Configure input & output ---------------------------------------------
  // --------------------------- Prepare input blobs -----------------------------------------------------
  ROS_INFO_STREAM("-------------------------------------------");
  ROS_INFO_STREAM("Checking that the inputs are as the ssd expects");

  InputsDataMap inputInfo(netReader.getNetwork().getInputsInfo());

  for (const auto & inputInfoItem : inputInfo) {
      if (inputInfoItem.second->getTensorDesc().getDims().size() == 4) {  // first input contains images
          imageInputName = inputInfoItem.first;
          inputInfoItem.second->setPrecision(Precision::U8);
          if (FLAGS_auto_resize) {
              inputInfoItem.second->getPreProcess().setResizeAlgorithm(ResizeAlgorithm::RESIZE_BILINEAR);
              inputInfoItem.second->getInputData()->setLayout(Layout::NHWC);
          } else {
              inputInfoItem.second->getInputData()->setLayout(Layout::NCHW);
          }
          const TensorDesc& inputDesc = inputInfoItem.second->getTensorDesc();
          netInputHeight = getTensorHeight(inputDesc);
          netInputWidth = getTensorWidth(inputDesc);
      } else if (inputInfoItem.second->getTensorDesc().getDims().size() == 2) {  // second input contains image info
          imageInfoInputName = inputInfoItem.first;
          inputInfoItem.second->setPrecision(Precision::FP32);
      } else {
          //ROS_ERROR_STREAM("Unsupported " << std::to_string(inputInfoItem.second->getTensorDesc().getDims().size()))
          //ROS_ERROR_STREAM("D input layer '" << inputInfoItem.first << "'. ");
          ROS_ERROR_STREAM("Only 2D and 4D input layers are supported");
      }
  }
  ROS_INFO_STREAM("---Successfully checked");
  // --------------------------- Prepare output blobs -----------------------------------------------------
  ROS_INFO_STREAM("Checking that the outputs are as the ssd expects");

  OutputsDataMap outputInfo(netReader.getNetwork().getOutputsInfo());

  if (outputInfo.size() != 1) {
      ROS_ERROR_STREAM("This program accepts networks having only one output");
  }
  DataPtr& output = outputInfo.begin()->second;
  outputName = outputInfo.begin()->first;
  const SizeVector outputDims = output->getTensorDesc().getDims();
  maxProposalCount = outputDims[2];
  objectSize = outputDims[3];
  if (objectSize != 7) {
      ROS_ERROR_STREAM("Output should have 7 as a last dimension");
  }
  if (outputDims.size() != 4) {
      ROS_ERROR_STREAM("Incorrect output dimensions for SSD");
  }
  output->setPrecision(Precision::FP32);
  output->setLayout(Layout::NCHW);
  ROS_INFO_STREAM("---Successfully checked");
  // -----------------------------------------------------------------------------------------------------
  is_telegram_online = false;
  // --------------------------- 4. Loading model to the device ------------------------------------------
  ROS_INFO_STREAM("-------------------------------------------");
  ROS_INFO_STREAM("Loading model to the device");
  network = ie.LoadNetwork(netReader.getNetwork(), DEVICE);
  ROS_INFO_STREAM("---Successfully loaded");
  // -----------------------------------------------------------------------------------------------------

  // --------------------------- 5. Create infer request -------------------------------------------------
  ROS_INFO_STREAM("-------------------------------------------");
  ROS_INFO_STREAM("Create infer request");

  async_infer_request_curr = network.CreateInferRequestPtr();
  async_infer_request_next = network.CreateInferRequestPtr();

  /* it's enough just to set image info input (if used in the model) only once */
  if (!imageInfoInputName.empty()) {
      auto setImgInfoBlob = [&](const InferRequest::Ptr &inferReq) {
          auto blob = inferReq->GetBlob(imageInfoInputName);
          auto data = blob->buffer().as<PrecisionTrait<Precision::FP32>::value_type *>();
          data[0] = static_cast<float>(netInputHeight);  // height
          data[1] = static_cast<float>(netInputWidth);  // width
          data[2] = 1;
      };
      setImgInfoBlob(async_infer_request_curr);
      setImgInfoBlob(async_infer_request_next);
  }
  ROS_INFO_STREAM("---Successfully created");
  // -----------------------------------------------------------------------------------------------------
  ROS_INFO_STREAM("-------------------------------------------");
  ROS_INFO_STREAM("Ready to do inference");
  ROS_INFO_STREAM("===========================================");

}

void ObjectDetection::frameToBlob(const cv::Mat& frame,
                 InferRequest::Ptr& inferRequest,
                 const std::string& inputName) {
    //if (true) {
        /* Just set input blob containing read image. Resize and layout conversion will be done automatically */
    //    inferRequest->SetBlob(inputName, wrapMat2Blob(frame));
  //  } else {
        /* Resize and copy data from the image to the input blob */
        Blob::Ptr frameBlob = inferRequest->GetBlob(inputName);
        matU8ToBlob<uint8_t>(frame, frameBlob);
  //  }
}


void ObjectDetection::objectInference()
{
  ROS_INFO_STREAM("-------------------------------------------");
  ROS_INFO_STREAM("Start inference");
  try{

  bool isLastFrame = false;
  bool isAsyncMode = true;

  typedef std::chrono::duration<double, std::ratio<1, 1000>> ms;
  auto wallclock = std::chrono::high_resolution_clock::now();

  while (true) {
        isTracking = false;
        auto t0 = std::chrono::high_resolution_clock::now();
        // Here is the first asynchronous point:
        // in the async mode we capture frame to populate the NEXT infer request
        if (!capture.read(next_frame)) {
            if (next_frame.empty()) {
                isLastFrame = true;  // end of video file
            } else {
                ROS_ERROR_STREAM("Failed to get frame from cv::VideoCapture");
            }
        }
        if (isAsyncMode) {
          if (!isLastFrame) {
            frameToBlob(next_frame, async_infer_request_next, imageInputName);
          }
        }

        t0 = std::chrono::high_resolution_clock::now();
        // Main sync point:
        // in the truly Async mode we start the NEXT infer request, while waiting for the CURRENT to complete
        if (isAsyncMode) {
            if (!isLastFrame) {
                async_infer_request_next->StartAsync();
            }
          }
        float max_area = 0;

        if (OK == async_infer_request_curr->Wait(IInferRequest::WaitMode::RESULT_READY)) {

            t0 = std::chrono::high_resolution_clock::now();
            ms wall = std::chrono::duration_cast<ms>(t0 - wallclock);
            wallclock = t0;

            t0 = std::chrono::high_resolution_clock::now();
            std::ostringstream out;
            out << "Wallclock time (TRUE ASYNC):";
            //ROS_INFO_STREAM(1000.f / wall.count());
            out << std::fixed << std::setprecision(2) << wall.count() << " ms (" << 1000.f / wall.count() << " fps)";
            cv::putText(curr_frame, out.str(), cv::Point2f(0, 50), cv::FONT_HERSHEY_TRIPLEX, 0.6, cv::Scalar(0, 0, 255));
            // ---------------------------Process output blobs--------------------------------------------------
            // Processing results of the CURRENT request

            int count;
            const float *detections = async_infer_request_curr->GetBlob(outputName)->buffer().as<PrecisionTrait<Precision::FP32>::value_type*>();
            for (int i = 0; i < maxProposalCount; i++) {
                float image_id = detections[i * objectSize + 0];
                if (image_id < 0) {
                    break;
                }

                float confidence = detections[i * objectSize + 2];
                auto label = static_cast<int>(detections[i * objectSize + 1]);
                xmin = detections[i * objectSize + 3] * input_width;
                ymin = detections[i * objectSize + 4] * input_height;
                xmax = detections[i * objectSize + 5] * input_width;
                ymax = detections[i * objectSize + 6] * input_height;

                left_points.push_back(cv::Point2f(xmin, ymin));
                right_points.push_back(cv::Point2f(xmax, ymax));


            if (confidence > MIN_CONFIDENCE) {
                /** Drawing only objects when > confidence_threshold probability **/
                cv::rectangle(curr_frame, cv::Point2f(xmin, ymin), cv::Point2f(xmax, ymax), greenColor, 2);
                // find the biggest box
                isTracking = true;

                float area_heigth = ymax - ymin;
                float area_width  = xmax - xmin;
                area = area_heigth * area_width;
                if (area > max_area)
                {
                  max_area = area;
                  count = i;
                }
              }
            }

          for (int i = 0; i < maxProposalCount; i++) {
            float confidence = detections[i * objectSize + 2];
            if (confidence > MIN_CONFIDENCE){
              centroid_coords = findCentroid(left_points[count],right_points[count]);

              centroid.status = isTracking;
              centroid.x_cent = centroid_coords.x;
              centroid.y_cent = centroid_coords.y;
              img_size = curr_frame.size();
              center_image_x = img_size.width / 2;
              center_image_y = img_size.height / 2;
              float area_image = img_size.width * img_size.height;
              nec_area = 0.0003 * area_image - area;
              cv::rectangle(curr_frame, cv::Point2f(left_points[count].x, left_points[count].y),
                                        cv::Point2f(right_points[count].x, right_points[count].y),
                                        greenColor, 2);

            }
          }
        }
        left_points.clear();
        right_points.clear();

        box_max.status = isTracking;
        box_max.x_left = xmin;
        box_max.y_left = ymin;
        box_max.x_right = xmax;
        box_max.y_right = ymax;

        deviation.status = isTracking;
        deviation.x_dev = nec_area;
        deviation.y_dev = center_image_x - centroid_coords.x;
        deviation.z_dev = center_image_y - centroid_coords.y;

        drawCentroid();
        //cv::imshow("Detection results", curr_frame);

        sensor_msgs::ImagePtr image_raw_msg = cv_bridge::CvImage(std_msgs::Header(), "bgr8", raw_frame).toImageMsg();
        sensor_msgs::ImagePtr image_msg = cv_bridge::CvImage(std_msgs::Header(), "bgr8", curr_frame).toImageMsg();

        image_raw_pub.publish(image_raw_msg);
        image_detected_pub.publish(image_msg);
        boxes_max_pub.publish(box_max);
        centroid_pub.publish(centroid);
        deviation_pub.publish(deviation);

        if (isLastFrame) {
            break;
        }

        // Final point:
        // in the truly Async mode we swap the NEXT and CURRENT requests for the next iteration
        curr_frame = next_frame;
        next_frame = cv::Mat();
        if (isAsyncMode) {
            async_infer_request_curr.swap(async_infer_request_next);
        }

        const int key = cv::waitKey(1);
        if (27 == key)  // Esc
            break;
        }
}
  catch (const std::exception& error) {
    ROS_ERROR_STREAM("[ ERROR ] " <<  error.what());
      return;
  }
  catch (...) {
    ROS_ERROR_STREAM("[ ERROR ] Unknown/internal exception happened.");
    return;
  }
cv::circle(curr_frame, centroid_coords, 4, cv::Scalar(255,255,255), 5, 8, 0);
  ROS_INFO_STREAM("Execution successful");
  return;

}

void ObjectDetection::drawCentroid(){
  cv::circle(curr_frame, centroid_coords, 4, cv::Scalar(255,255,255), 5, 8, 0);
  // draw the center by lines
  cv::line(curr_frame, cv::Point(0, center_image_y), cv::Point(img_size.width, center_image_y), whiteColor,  1, 8);
  cv::line(curr_frame, cv::Point(center_image_x, 0), cv::Point(center_image_x, img_size.height), whiteColor, 1, 8);
}


cv::Point2f ObjectDetection::findCentroid(cv::Point2f left_point,cv::Point2f right_point)
{
  float centroid_x = (right_point.x + left_point.x)/2;
  float centroid_y = (right_point.y + left_point.y)/2;
  return cv::Point2f(centroid_x, centroid_y);
}


int main(int argc, char *argv[]) {

  ros::init(argc, argv, "object_detection_node");
  cout.flush();

  while (ros::ok())
  {
    ObjectDetection obj;
    ros::Rate r(10);
    obj.objectInference();
    ros::spinOnce();
    r.sleep();
  }
    return 0;
}
